import Fastify from 'fastify';
import WebSocket from 'ws';
import dotenv from 'dotenv';
import fastifyFormBody from '@fastify/formbody';
import fastifyWs from '@fastify/websocket';

// Load environment variables from .env file
dotenv.config();

// Retrieve the OpenAI API key from environment variables.
const { OPENAI_API_KEY } = process.env;

if (!OPENAI_API_KEY) {
    console.error('Missing OpenAI API key. Please set it in the .env file.');
    process.exit(1);
}

// Initialize Fastify
const fastify = Fastify();
fastify.register(fastifyFormBody);
fastify.register(fastifyWs);

// Constants
const SYSTEM_MESSAGE = 'You are a helpful and bubbly AI assistant who loves to chat about anything the user is interested about and is prepared to offer them facts. You have a penchant for dad jokes, owl jokes, and rickrolling â€“ subtly. Always stay positive, but work in a joke when appropriate.';
const VOICE = 'alloy';
const PORT = process.env.PORT || 5050; // Allow dynamic port assignment


// List of Event Types to log to the console. See the OpenAI Realtime API Documentation: https://platform.openai.com/docs/api-reference/realtime
const LOG_EVENT_TYPES = [
    'response.content.done',
    'rate_limits.updated',
    'response.done',
    'input_audio_buffer.committed',
    'input_audio_buffer.speech_stopped',
    'input_audio_buffer.speech_started',
    'session.created',
    // Added events for transcription
    'conversation.item.input_audio_transcription.completed',
    'response.text.delta',
    'response.text.done',
    'response.audio_transcript.delta',
    'response.audio_transcript.done'
];

// Track drift between OpenAI and system clocks, and the assistant's last Item ID
let localStartTime;
let lastDrift = null;
let lastAssistantItem;

// Initialize transcription storage
let userTranscription = '';
let assistantTranscription = '';


// Root Route
fastify.get('/', async (request, reply) => {
    reply.send({ message: 'Twilio Media Stream Server is running!' });
});

// Route for Twilio to handle incoming and outgoing calls
// <Say> punctuation to improve text-to-speech translation
fastify.all('/incoming-call', async (request, reply) => {
   

    const twimlResponse = `<?xml version="1.0" encoding="UTF-8"?>
                          <Response>
                              <Say>Please wait while we connect your call to the A. I. voice assistant, powered by Twilio and the Open-A.I. Realtime API</Say>
                              <Pause length="1"/>
                              <Say>O.K. you can start talking!</Say>
                              <Connect>
                                  <Stream url="wss://${request.headers.host}/media-stream" />
                              </Connect>
                          </Response>`;

    reply.type('text/xml').send(twimlResponse);
});

// WebSocket route for media-stream
fastify.register(async (fastify) => {


    fastify.get('/media-stream', { websocket: true }, (connection, req) => {

        console.log('Client connected');
        console.log(`Incoming call from ${req.raw.url}`);

        const openAiWs = new WebSocket('wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01', {
            headers: {
                Authorization: `Bearer ${OPENAI_API_KEY}`,
                "OpenAI-Beta": "realtime=v1"
            }
        });

        let streamSid = null;
        let callSid = null;
        let sessionMessage = null;

        const initializeSession = () => {
            const sessionUpdate = {
                type: 'session.update',
                session: {
                    turn_detection: { type: 'server_vad' },
                    input_audio_format: 'g711_ulaw',
                    output_audio_format: 'g711_ulaw',
                    voice: VOICE,
                    instructions: SYSTEM_MESSAGE,
                    modalities: ["text", "audio"],
                    temperature: 0.8,
                }
            };

            console.log('Sending session update:', JSON.stringify(sessionUpdate));
            openAiWs.send(JSON.stringify(sessionUpdate));

            // Uncomment the following line to have AI speak first:
            sendInitialConversationItem();
        };

        const sendInitialConversationItem = () => {
            const initialConversationItem = {
                type: 'conversation.item.create',
                item: {
                    type: 'message',
                    role: 'user',
                    content: [
                        {
                            type: 'input_text',
                            text: 'Greet the user with "Hello there! I am an AI voice assistant powered by Twilio and the OpenAI Realtime API. You can ask me for facts, jokes, or anything you can imagine. How can I help you?"'
                        }
                    ]
                }
            };

            console.log('Sending initial conversation item:', JSON.stringify(initialConversationItem));
            openAiWs.send(JSON.stringify(initialConversationItem));
            openAiWs.send(JSON.stringify({ type: 'response.create' }));
        };

        // Open event for OpenAI WebSocket
        openAiWs.on('open', () => {
            localStartTime = Date.now(); // Start local timer
            console.log('Connected to the OpenAI Realtime API');
            setTimeout(initializeSession, 100);
        });

         // Define the missing functions
         const handleSpeechStartedEvent = (response) => {
            const localTime = Date.now();
            const drift = localTime - localStartTime - response.audio_start_ms;

            console.log('OpenAI Speech started at', response.audio_start_ms, 'ms from OpenAI perspective');
            console.log('Local time at speech start:', localTime - localStartTime, 'ms');
            console.log('Time drift (OpenAI - Local):', drift, 'ms');

            if (lastDrift === null || drift !== lastDrift) {
                console.log('Drift has changed. Previous:', lastDrift, 'Current:', drift);
                lastDrift = drift;
            }

            if (streamSid) {
                connection.send(JSON.stringify({
                    event: 'clear',
                    streamSid: streamSid
                }));
            }

            if (lastAssistantItem) {
                const truncateEvent = {
                    type: 'conversation.item.truncate',
                    item_id: lastAssistantItem,
                    content_index: 0,
                    audio_end_ms: response.audio_start_ms
                };
                console.log('Sending truncation event:', JSON.stringify(truncateEvent) );
                openAiWs.send(JSON.stringify(truncateEvent));
                lastAssistantItem = null;
            }
        };

        const handleResponseDoneEvent = (response) => {
            const outputItems = response.response.output;
            for (const item of outputItems) {
                if (item.role === 'assistant') {
                    lastAssistantItem = item.id;
                    break; // Consider the first relevant assistant item
                }
            }
        };


        // Listen for messages from the OpenAI WebSocket (and send to Twilio if necessary)
        openAiWs.on('message', (data) => {
            try {
                const response = JSON.parse(data);

                if (LOG_EVENT_TYPES.includes(response.type)) {
                    console.log(`Received event: ${response.type}`, response);
                }

                if (response.type === 'session.updated') {
                    console.log('Session updated successfully:', response);
                }

                 // Capture user transcription
                 if (response.type === 'conversation.item.input_audio_transcription.completed') {
                    const transcription = response.item.content.find(c => c.type === 'text').text;
                    console.log('User transcription:', transcription);
                    // Append to user transcription
                    userTranscription += transcription + '\n';
                }

                // Capture assistant's transcription in real-time
                if (response.type === 'response.text.delta' && response.delta) {
                    //console.log('Assistant transcription delta:', response.delta);
                    // Append delta to assistant transcription
                    assistantTranscription += response.delta;
                }

                if (response.type === 'response.audio.delta' && response.delta) {
                    const audioDelta = {
                        event: 'media',
                        streamSid: streamSid,
                        media: { payload: Buffer.from(response.delta, 'base64').toString('base64') }
                    };
                    connection.send(JSON.stringify(audioDelta));
                }

                // When assistant's response is done
                if (response.type === 'response.text.done') {
                    console.log('Assistant transcription done:', assistantTranscription);
                    // You can process or store the assistantTranscription here
                    assistantTranscription += '\n';
                }

                if (response.type === 'session.updated') {
                    console.log('Session updated successfully:', response);
                }               

                // We can get the following event while Twilio is still playing audio from the AI
                if (response.type === 'input_audio_buffer.speech_started') {
                    handleSpeechStartedEvent(response);
                }            

                // We can get the following event while Twilio is still playing audio from the AI
                if (response.type === 'input_audio_buffer.speech_started') {
                    handleSpeechStartedEvent(response);
                }
                if (response.type === 'response.done') {
                    handleResponseDoneEvent(response);
                }
            } catch (error) {
                console.error('Error processing OpenAI message:', error, 'Raw message:', data);
            }
        });

      



        // Handle incoming messages from Twilio
        connection.on('message', (message) => {
            try {
                const data = JSON.parse(message);

                switch (data.event) {
                    case 'media':
                        if (openAiWs.readyState === WebSocket.OPEN) {
                            const audioAppend = {
                                type: 'input_audio_buffer.append',
                                audio: data.media.payload
                            };
                            openAiWs.send(JSON.stringify(audioAppend));
                        }
                        break;

                    case 'connected':
                  
                    case 'start':
                        streamSid = data.start.streamSid;
                        console.log('Incoming stream has started', streamSid);

                        callSid = data.start.callSid;
                        console.log('CallSid:', callSid); 
                        break;
                    default:
                        console.log('Received non-media event:', data.event);
                        break;
                }
            } catch (error) {
                console.error('Error parsing message:', error, 'Message:', message);
            }
        });

       // Handle connection close
        connection.on('close', () => {
        if (openAiWs.readyState === WebSocket.OPEN) openAiWs.close();
        console.log('Client disconnected.');

        // Output the final transcriptions
        console.log('Final User Transcription:\n', userTranscription);
        console.log('Final Assistant Transcription:\n', assistantTranscription);
});


        // Handle WebSocket close and errors
        openAiWs.on('close', () => {
            console.log('Disconnected from the OpenAI Realtime API');
        });

        openAiWs.on('error', (error) => {
            console.error('Error in the OpenAI WebSocket:', error);
        });
    });
});


const lookupSessionMessage = async (callSid) => {
    try {
        // Implement your lookup logic here, e.g., database query
        const sessionMessage = await yourLookupFunction(callSid);
        
        if (sessionMessage) {
            const sessionUpdate = {
                type: 'session.update',
                session: {
                    // ... other session details
                    instructions: sessionMessage,
                }
            };
            console.log('Sending custom session update:', JSON.stringify(sessionUpdate));
            openAiWs.send(JSON.stringify(sessionUpdate));
        }
    } catch (error) {
        console.error('Error looking up session message:', error);
    }
};

fastify.listen({ port: PORT, host: '0.0.0.0' }, (err) => {
    if (err) {
        console.error(err);
        process.exit(1);
    }
    console.log(`Server is listening on port ${PORT}`);
});